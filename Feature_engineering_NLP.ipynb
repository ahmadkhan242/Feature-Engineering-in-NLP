{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ahmadkhan242/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ahmadkhan242/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "val = pd.read_csv(\"validation.csv\")\n",
    "test = pd.read_csv(\"testWithLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating training and validation\n",
    "df = pd.concat([df, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Pre-processing \n",
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@'\n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+my_punctuation + ']+', ' ', sent) # strip punctuation\n",
    "    sent = re.sub('\\s+', ' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x: preprocess(x))\n",
    "test['tweet']=test['tweet'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8560, 13158)\n",
      "(2140, 13158)\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_final_features = vectorizer.fit_transform(df['tweet']).toarray()\n",
    "test_final_features = vectorizer.transform(test['tweet']).toarray()\n",
    "print(train_final_features.shape)\n",
    "print(test_final_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf= pd.DataFrame(train_final_features)\n",
    "test_tf_idf= pd.DataFrame(test_final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = df['label']\n",
    "test_label = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_tf_idf, train_label ,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tf_idf, train_label ,test_size=0.2)\n",
    "_RandomForestClassifier = RandomForestClassifier(n_estimators = 1000, min_samples_split = 15, random_state = 42)\n",
    "_RandomForestClassifier.fit(X_train, y_train)\n",
    "_RandomForestClassifier_prediction = _RandomForestClassifier.predict(X_test)\n",
    "val_RandomForestClassifier_prediction = _RandomForestClassifier.predict(test_tf_idf)\n",
    "print(\"Accuracy => \", round(accuracy_score(_RandomForestClassifier_prediction, y_test)*100, 2))\n",
    "print(\"\\nRandom Forest Classifier results: \\n\")\n",
    "print(classification_report(y_test, _RandomForestClassifier_prediction, target_names = ['real', 'fake']))\n",
    "print(\"Validation Accuracy => \", round(accuracy_score(val_RandomForestClassifier_prediction, ytest)*100, 2))\n",
    "print(\"\\nValidation Random Forest Classifier results: \\n\")\n",
    "print(classification_report(ytest, val_RandomForestClassifier_prediction, target_names = ['real', 'fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of characters \n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "# count number of words \n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# count number of capital characters\n",
    "def count_capital_chars(text):\n",
    "    count=0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "# count number of capital words\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))\n",
    "\n",
    "# count number of punctuations\n",
    "def count_punctuations(text):\n",
    "    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d\n",
    "\n",
    "# count number of words in quotes\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(\"\\'.\\'|\\\".\\\"\", text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count\n",
    "    \n",
    "# count number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "# calculate average word length\n",
    "def avg_word_len(char_cnt,word_cnt):\n",
    "    return char_cnt/word_cnt\n",
    "\n",
    "# calculate average sentence length\n",
    "def avg_sent_len(word_cnt,sent_cnt):\n",
    "    return word_cnt/sent_cnt\n",
    "\n",
    "# count number of unique words \n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "            \n",
    "# words vs unique feature\n",
    "def words_vs_unique(words,unique):\n",
    "    return unique/words\n",
    "    \n",
    "# count of hashtags\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(\\#\\w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "# count of mentions\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(\\@\\w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "# count of stopwords\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n",
    "# stopwords vs words\n",
    "def stopwords_vs_words(stopwords_cnt,text):\n",
    "    return stopwords_cnt/len(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "val = pd.read_csv(\"validation.csv\")\n",
    "test = pd.read_csv(\"testWithLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'] = df[\"tweet\"].apply(lambda x:count_chars(x))\n",
    "df['word_count'] = df[\"tweet\"].apply(lambda x:count_words(x))\n",
    "df['sent_count'] = df[\"tweet\"].apply(lambda x:count_sent(x))\n",
    "df['capital_char_count'] = df[\"tweet\"].apply(lambda x:count_capital_chars(x))\n",
    "df['capital_word_count'] = df[\"tweet\"].apply(lambda x:count_capital_words(x))\n",
    "df['quoted_word_count'] = df[\"tweet\"].apply(lambda x:count_words_in_quotes(x))\n",
    "df['stopword_count'] = df[\"tweet\"].apply(lambda x:count_stopwords(x))\n",
    "df['unique_word_count'] = df[\"tweet\"].apply(lambda x:count_unique_words(x))\n",
    "df['htag_count'] = df[\"tweet\"].apply(lambda x:count_htags(x))\n",
    "df['mention_count'] = df[\"tweet\"].apply(lambda x:count_mentions(x))\n",
    "df['punct_count'] = df[\"tweet\"].apply(lambda x:count_punctuations(x))\n",
    "df['avg_wordlength']=df['char_count']/df['word_count']\n",
    "df['avg_sentlength']=df['word_count']/df['sent_count']\n",
    "df['unique_vs_words']=df['unique_word_count']/df['word_count']\n",
    "df['stopwords_vs_words']=df['stopword_count']/df['word_count']\n",
    "\n",
    "# SIMILARLY YOU CAN APPLY THEM ON TEST SET\n",
    "test['char_count'] = test[\"tweet\"].apply(lambda x:count_chars(x))\n",
    "test['word_count'] = test[\"tweet\"].apply(lambda x:count_words(x))\n",
    "test['sent_count'] = test[\"tweet\"].apply(lambda x:count_sent(x))\n",
    "test['capital_char_count'] = test[\"tweet\"].apply(lambda x:count_capital_chars(x))\n",
    "test['capital_word_count'] = test[\"tweet\"].apply(lambda x:count_capital_words(x))\n",
    "test['quoted_word_count'] = test[\"tweet\"].apply(lambda x:count_words_in_quotes(x))\n",
    "test['stopword_count'] = test[\"tweet\"].apply(lambda x:count_stopwords(x))\n",
    "test['unique_word_count'] = test[\"tweet\"].apply(lambda x:count_unique_words(x))\n",
    "test['htag_count'] = test[\"tweet\"].apply(lambda x:count_htags(x))\n",
    "test['mention_count'] = test[\"tweet\"].apply(lambda x:count_mentions(x))\n",
    "test['punct_count'] = test[\"tweet\"].apply(lambda x:count_punctuations(x))\n",
    "test['avg_wordlength']=test['char_count']/test['word_count']\n",
    "test['avg_sentlength']=test['word_count']/test['sent_count']\n",
    "test['unique_vs_words']=test['unique_word_count']/test['word_count']\n",
    "test['stopwords_vs_words']=test['stopword_count']/test['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_punct= pd.DataFrame(list(df.punct_count))\n",
    "test_punct= pd.DataFrame(list(test.punct_count))\n",
    "\n",
    "# Mearning pnctuation DataFrame with main DataFrame\n",
    "df=pd.merge(df,df_punct,left_index=True, right_index=True)\n",
    "test=pd.merge(test,test_punct,left_index=True, right_index=True)\n",
    "\n",
    "# We can drop \"punct_count\" column from both df and test DataFrame\n",
    "df.drop(columns=['punct_count'],inplace=True)\n",
    "test.drop(columns=['punct_count'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(lambda x: preprocess(x))\n",
    "test['tweet']=test['tweet'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['punct_count'],inplace=True)\n",
    "test.drop(columns=['punct_count'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].astype('string')\n",
    "test['tweet'] = test['tweet'].astype('string')\n",
    "df['tweet'].fillna('', inplace = True)\n",
    "test['tweet'].fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer            =  TfidfVectorizer()\n",
    "train_tf_idf_features =  vectorizer.fit_transform(df['tweet']).toarray()\n",
    "test_tf_idf_features  =  vectorizer.transform(test['tweet']).toarray()\n",
    "# Converting above list to DataFrame\n",
    "train_tf_idf          = pd.DataFrame(train_tf_idf_features)\n",
    "test_tf_idf           = pd.DataFrame(test_tf_idf_features)\n",
    "# Saparating train and test labels from all features\n",
    "train_Y               = df['label']\n",
    "test_Y                = test['label']\n",
    "# Listing all features\n",
    "features = ['char_count', 'word_count', 'sent_count',\n",
    "       'capital_char_count', 'capital_word_count', 'quoted_word_count',\n",
    "       'stopword_count', 'unique_word_count', 'htag_count', 'mention_count',\n",
    "       'avg_wordlength', 'avg_sentlength', 'unique_vs_words',\n",
    "       'stopwords_vs_words', '! count', '\" count', '# count', '$ count',\n",
    "       '% count', '& count', '\\' count', '( count', ') count', '* count',\n",
    "       '+ count', ', count', '- count', '. count', '/ count', ': count',\n",
    "       '; count', '< count', '= count', '> count', '? count', '@ count',\n",
    "       '[ count', '\\ count', '] count', '^ count', '_ count', '` count',\n",
    "       '{ count', '| count', '} count', '~ count']\n",
    "# Finally merging all features with above TF-IDF. \n",
    "train = pd.merge(train_tf_idf,df[features],left_index=True, right_index=True)\n",
    "test  = pd.merge(test_tf_idf,test[features],left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, train_Y, test_size=0.2, random_state = 42)# Random Forest Classifier\n",
    "_RandomForestClassifier = RandomForestClassifier(n_estimators = 1000, min_samples_split = 15, random_state = 42)\n",
    "_RandomForestClassifier.fit(X_train, y_train)\n",
    "_RandomForestClassifier_prediction = _RandomForestClassifier.predict(X_test)\n",
    "val_RandomForestClassifier_prediction = _RandomForestClassifier.predict(test)\n",
    "print(\"Accuracy => \", round(accuracy_score(_RandomForestClassifier_prediction, y_test)*100, 2))\n",
    "print(\"\\nRandom Forest Classifier results: \\n\")\n",
    "print(classification_report(y_test, _RandomForestClassifier_prediction, target_names = ['real', 'fake']))\n",
    "print(\"Validation Accuracy => \", round(accuracy_score(val_RandomForestClassifier_prediction, test_Y)*100, 2))\n",
    "print(\"\\nValidation Random Forest Classifier results: \\n\")\n",
    "print(classification_report(test_Y, val_RandomForestClassifier_prediction, target_names = ['real', 'fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
